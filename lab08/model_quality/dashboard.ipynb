{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лаба 8. Мониторинг качества работы модели машинного обучения с использованием дэшбордов\n",
    "\n",
    "Итак, у нас есть модель и предсказания пола и возрастной группы по ней. Мы можем посчитать метрику модели, зная метки тестового датасета. Однако в проме у нас нет истинных меток, и надо как-то понимать, модель дает нам то качество предсказаний, на которое мы расчитываем? Не деградирует ли она со временем?\n",
    "\n",
    "К этой проблеме подходят по разному.\n",
    "\n",
    "1. Бизнес метрика более высокого уровня.\n",
    "\n",
    "Мы не может посчитать точность предсказания модели, но мы можем посчитать эффект применения модели. Например, мы определяем пол и возраст новых пользователей, чтобы сделать им таргетированное предложение в рамках маркетинговой кампании. Тогда мы можем подсчитать эффективность кампании с таргетированием через коэффициент конверсии, то есть сколько людей приняло предложение. Конечно, наша модель тут только один из факторов успеха кампании, но все же, стат. методами мы сможем оценить вклад и нашей модели. Например, можем сравнивать конверсию клиентов таргетированной рекламы с полностью случайной рекламной рассылкой. Или сравнивания с разультатами работы другой модели.\n",
    "\n",
    "2. А/Б тестирование\n",
    "\n",
    "Мы сравниваем работу модели на двух группах: одна – обычные рандомные пользователи, другая – контрольная – состоит из пользователей, которых мы знаем.\n",
    "\n",
    "3. Смотрим на внутренние показатели работы модели.\n",
    "\n",
    "К таковым относится, например, распределение предсказаний модели по классам. Если оно начинает сдвигаться со временем, то возможно, что надо переучивать модель на свежих данных.\n",
    "\n",
    "## I. Задача с высоты птичьего полета\n",
    "\n",
    "Давайте проиллюстрируем последний способ. Вы запишите ваши предсказания в ElasticSearch и далее построите dashboard с графиками изменений числа предсказаний со временем. По линиям тренда будет видно, дрейфует ли модель.\n",
    "\n",
    "## II. Реализация\n",
    "\n",
    "Возмите 5000 предсказаний, которые у вас есть после выполнения Лабы 7. Разбейте их на 25 частей. Будем предполагать, что каждая часть – предсказания за определенный день, начиная с 1-го июня 2020. \n",
    "\n",
    "Измените ваше приложение из Лабы 7а таким образом, чтобы записывать предсказания в индекс Elasticsearch под названием `name_surname_lab08`. Помимо предсказаний, в сообщениях также должно содержаться поле `date` с временной меткой в миллисекундах эпохи, назначенной в соответствии с описанным выше – то есть первым 200 сообщениям должна назначаться метка 1-го июня, следующим – 2-го июня и т.д. Перед тем как записывать события в Elastic, создайте индекс используя REST API. Смотрите короткую [справку по АПИ Elasticsearch и Kibana](Elastic_API.md).\n",
    "\n",
    "Постройте в Кибане график (visualization) с числом предсказаний каждого класса в предсказаниях в зависимости от времени.  \n",
    "\n",
    "Для этого перейдите в Kibana по адресу 10.0.1.9:5601, в пункте меню Visualization выберите тип `timelion`, и в качестве `timelion expression` введите: `.es(index=name_surname_lab08, metric=count, timefield=date, split=gender_age:10)`/ Сохраните под именем `name_surname_lab08`.\n",
    "\n",
    "Посмотрите, меняется ли распределение классов со временем? \n",
    "\n",
    "График должен называться `name_surname_lab08` и выглядеть примерно так:\n",
    "\n",
    "![lab08.png](img/lab08.png)\n",
    "\n",
    "Далее постройте график с трендами придсказаний каждого класса. Воспользуйтесь `.trend()`. Повторите процедуру, только добавьте к концу выражения `.trend()`. Этот график должен называться `name_surname_lab08_trend` и выглядеть он должен примерно так:\n",
    "\n",
    "![lab08_trend.png](img/lab08_trend.png)\n",
    "\n",
    "Посмотрите, есть ли тренды? В качестве самостоятельного задания подсчитайте статистическими методами, можно ли сделать вывод о наличии тренда? Т.е. примите в качестве нулевой гипотезы, что тренда нет, а в качестве альтернативной, что он есть. Посчитайте p-value.\n",
    "\n",
    "Далее создайте dashboard в пункте меню `Dashboard` под названием `name_surname_lab08` и добавьте туда оба графика.\n",
    "\n",
    "## III. Оформление работы\n",
    "\n",
    "Ваш проект в репо в подпапке lab08 должен называться `model_quality`.\n",
    "\n",
    "## IV. Доступ к Elastic и Kibana\n",
    "\n",
    "* Elasticsearch REST API: 10.0.1.9:9200\n",
    "* Kibana Web UI, REST API: 10.0.1.9:5601 \n",
    "\n",
    "Для логина в Web UI и аутентикации REST API используйте ваш логин и пароль в ЛК. Web UI доступен с пробросом порта по туннелю или через socks-прокси. Авторизация аккаунтов настроена таким образом, что вы можете создавать индексы с шаблоном name_surname*, и не имеете доступа ни к каким другим индексам.\n",
    "\n",
    "## V. Проверка\n",
    "\n",
    "Чекер найдет ваш dashboard, скачает его в формате `json`, и проверит.\n",
    "\n",
    "### Поля чекера\n",
    "\n",
    "* `git_correct` – проверка репо\n",
    "* `git_errors` – ошибки репо\n",
    "* `index_correct` – в Elasticsearch имеется индекс `name_surname_lab08` с правильными полями\n",
    "* `dashboard_correct` – в Kibana имеется dashboard `name_surname_lab08` и он правильный.\n",
    "* `lab_result`\n",
    "\n",
    "### Cамопроверка\n",
    "\n",
    "#### Поиск dashboard\n",
    "\n",
    "`curl -X GET 10.0.1.9:5601/api/saved_objects/_find?type=dashboard&search=artem_trunov_lab08`\n",
    "\n",
    "#### Считывание dashboard\n",
    "\n",
    "`curl -X GET 10.0.1.9:5601/api/kibana/dashboards/export?dashboard=d818cd30-a985-11ea-8889-8de7ce1ad0f9`\n",
    "\n",
    "В вашем dashboard должно быть три объекта - один с типом dashboard и названием name_surname_lab08, и два с типом visualization и названиями name_surname_lab08, name_surname_lab08_trend.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "conf = org.apache.spark.SparkConf@5d693551\n",
       "sparkSession = org.apache.spark.sql.SparkSession@20092f9a\n",
       "sc = org.apache.spark.SparkContext@50eacc13\n",
       "sqlContext = org.apache.spark.sql.SQLContext@1bd92c57\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one deprecation warning; re-run with -deprecation for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SQLContext@1bd92c57"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.{SparkContext, SparkConf}\n",
    "import org.apache.spark.sql.{SQLContext, DataFrame}\n",
    "import org.apache.spark.sql.functions.{col, lit, current_timestamp, explode, to_timestamp, callUDF, regexp_replace\n",
    "                                           , from_unixtime, collect_list}\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "\n",
    "val conf = new SparkConf()\n",
    "                            .setAppName(\"lab08\")\n",
    "\n",
    "val sparkSession = SparkSession.builder()\n",
    "  .config(conf=conf)\n",
    "  .getOrCreate()\n",
    "\n",
    "var sc = sparkSession.sparkContext\n",
    "val sqlContext = new SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_path = /user/kirill.likhouzov/lab07/\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "/user/kirill.likhouzov/lab07/"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model_path = \"/user/kirill.likhouzov/lab07/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download from file:///data/home/kirill.likhouzov/Drivers/elasticsearch-spark-20_2.11-7.6.2.jar\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished download of elasticsearch-spark-20_2.11-7.6.2.jar\n"
     ]
    }
   ],
   "source": [
    "%AddJar file:///data/home/kirill.likhouzov/Drivers/elasticsearch-spark-20_2.11-7.6.2.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+--------------------+\n",
      "|         date|                 uid|              visits|\n",
      "+-------------+--------------------+--------------------+\n",
      "|1590969600000|0000e7ca-32e6-4be...|[[1419929378563, ...|\n",
      "+-------------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dataset = [date: bigint, uid: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<console>:6: error: Symbol 'type scala.AnyRef' is missing from the classpath.\n",
       "This symbol is required by 'class org.apache.spark.sql.catalyst.QualifiedTableName'.\n",
       "Make sure that type AnyRef is in your classpath and check for conflicting dependencies with `-Ylog-classpath`.\n",
       "A full rebuild may help if 'QualifiedTableName.class' was compiled against an incompatible version of scala.\n",
       "  lazy val $print: String =  {\n",
       "           ^\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[date: bigint, uid: string ... 1 more field]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dataset = spark\n",
    "    .read\n",
    "    .json(\"/labs/lab08/lab04_test5000_with_date_lab08.json\")\n",
    "    .repartition(1)\n",
    "\n",
    "dataset.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+-------------------+--------------+\n",
      "|                 uid|         date|       datetime_web|  host_not_www|\n",
      "+--------------------+-------------+-------------------+--------------+\n",
      "|0000e7ca-32e6-4be...|1590969600000|2014-12-30 11:49:38|hotelcosmos.ru|\n",
      "|0000e7ca-32e6-4be...|1590969600000|2014-12-30 11:48:43|hotelcosmos.ru|\n",
      "|0000e7ca-32e6-4be...|1590969600000|2014-12-17 18:52:17|business101.ru|\n",
      "+--------------------+-------------+-------------------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dataset_host = [uid: string, date: bigint ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[uid: string, date: bigint ... 2 more fields]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dataset_host = dataset.select($\"uid\"\n",
    "                                ,$\"date\"\n",
    "                                ,explode($\"visits\").as(\"web\"))  \n",
    "                        .withColumn(\"timestamp\", $\"web.timestamp\")\n",
    "                        .withColumn(\"url\", $\"web.url\")\n",
    "                        .withColumn(\"host\", callUDF(\"parse_url\", $\"url\", lit(\"HOST\")))\n",
    "                        .select($\"uid\"\n",
    "                                ,$\"date\"\n",
    "                                ,to_timestamp(from_unixtime($\"timestamp\" / 1000)).as(\"datetime_web\")\n",
    "                                ,regexp_replace($\"host\", \"www.\", \"\").as(\"host_not_www\"))\n",
    "\n",
    "dataset_host.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000+--------------------+-------------+--------------------+\n",
      "|                 uid|         date|             domains|\n",
      "+--------------------+-------------+--------------------+\n",
      "|01bfb888-2e76-49d...|1591056000000|[dns-shop.ru, dns...|\n",
      "|0201e7fd-4d86-450...|1591142400000|[privatehomeclips...|\n",
      "|02d13538-4768-4e5...|1591228800000|       [elementy.ru]|\n",
      "+--------------------+-------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "test = [uid: string, date: bigint ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[uid: string, date: bigint ... 1 more field]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val test = dataset_host.groupBy(\"uid\", \"date\")\n",
    "                        .agg(collect_list(\"host_not_www\").as(\"domains\"))\n",
    "                        .repartition(1)\n",
    "\n",
    "print(test.count)\n",
    "test.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+----------+\n",
      "|                 uid|         date|gender_age|\n",
      "+--------------------+-------------+----------+\n",
      "|01bfb888-2e76-49d...|1591056000000|   M:25-34|\n",
      "|0201e7fd-4d86-450...|1591142400000|   M:25-34|\n",
      "|02d13538-4768-4e5...|1591228800000|   M:25-34|\n",
      "+--------------------+-------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model = pipeline_25c0402f8ea6\n",
       "prediction = [uid: string, date: bigint ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<console>:6: error: Symbol 'type scala.AnyRef' is missing from the classpath.\n",
       "This symbol is required by 'trait org.apache.spark.ml.tree.DecisionTreeRegressorParams'.\n",
       "Make sure that type AnyRef is in your classpath and check for conflicting dependencies with `-Ylog-classpath`.\n",
       "A full rebuild may help if 'DecisionTreeRegressorParams.class' was compiled against an incompatible version of scala.\n",
       "  lazy val $print: String =  {\n",
       "           ^\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[uid: string, date: bigint ... 1 more field]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model = PipelineModel\n",
    "                        .load(model_path)\n",
    "\n",
    "val prediction  = model\n",
    "                        .transform(test)\n",
    "                        .select($\"uid\"\n",
    "                                ,$\"date\"\n",
    "                                ,$\"gender_age_pred\".as(\"gender_age\"))\n",
    "                        .repartition(1)\n",
    "\n",
    "prediction.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "esOptions = Map(es.nodes -> 10.0.1.9:9200/kirill_likhouzov_lab08, es.batch.write.refresh -> false, es.nodes.wan.only -> true)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map(es.nodes -> 10.0.1.9:9200/kirill_likhouzov_lab08, es.batch.write.refresh -> false, es.nodes.wan.only -> true)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val esOptions = \n",
    "    Map(\n",
    "        \"es.nodes\" -> \"10.0.1.9:9200/kirill_likhouzov_lab08\", \n",
    "        \"es.batch.write.refresh\" -> \"false\",\n",
    "        \"es.nodes.wan.only\" -> \"true\"   \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction\n",
    "    .write\n",
    "    .format(\"org.elasticsearch.spark.sql\")\n",
    "    .options(esOptions)\n",
    "    .save(\"kirill_likhouzov_lab08-{date}/_doc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- gender_age: string (nullable = true)\n",
      " |-- uid: string (nullable = true)\n",
      "\n",
      "-RECORD 0------------------------------------------\n",
      " date       | 2020-06-02 03:00:00                  \n",
      " gender_age | M:25-34                              \n",
      " uid        | 01bfb888-2e76-49d6-965c-4a11bcc164dd \n",
      "only showing top 1 row\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "esDf = [date: timestamp, gender_age: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[date: timestamp, gender_age: string ... 1 more field]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val esDf = spark\n",
    "                .read\n",
    "                .format(\"es\")\n",
    "                .options(esOptions)\n",
    "                .load(\"kirill_likhouzov_lab08-*\")\n",
    "esDf.printSchema\n",
    "esDf.show(1, 200, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "esDf.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// ssh -i npl.pem -L 5601:10.0.1.9:5601 kirill.likhouzov@spark-de-master1.newprolab.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// // удаление индекса\n",
    "// curl -X DELETE http://kirill.likhouzov:password@10.0.1.9:9200/kirill_likhouzov_lab08-*\n",
    "\n",
    "// // просмотр индекса\n",
    "// curl http://kirill.likhouzov:password@10.0.1.9:9200/kirill_likhouzov_lab08-*\n",
    "\n",
    "// // проверка дашборда\n",
    "// curl -X GET 10.0.1.9:5601/api/kibana/dashboards/export?dashboard=6bd54520-b92c-11ea-8889-8de7ce1ad0f9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// PUT _template/kirill_likhouzov_lab08\n",
    "// {\n",
    "//   \"index_patterns\": [\"kirill_likhouzov_lab08-*\"],\n",
    "//   \"settings\": {\n",
    "//     \"number_of_shards\": 1,\n",
    "//     \"number_of_replicas\" : 1\n",
    "//   },\n",
    "//   \"mappings\": {\n",
    "//     \"_doc\": {\n",
    "//       \"dynamic\": true,\n",
    "//       \"_source\": {\n",
    "//         \"enabled\": true\n",
    "//       },\n",
    "//       \"properties\": {\n",
    "//         \"uid\": {\n",
    "//         \"type\": \"keyword\"\n",
    "//           },\n",
    "//           \"gender_age\": {\n",
    "//             \"type\": \"keyword\"\n",
    "//           },\n",
    "//           \"date\": {\n",
    "//             \"type\": \"date\",\n",
    "//             \"format\": \"strict_date_optional_time||epoch_millis\"\n",
    "//           }\n",
    "//       }\n",
    "//     }\n",
    "//   }\n",
    "// }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
